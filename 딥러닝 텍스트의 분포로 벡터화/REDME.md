# 딥러닝 텍스트의 분포로 벡터화_노드학습 0616

이 리포지토리는 텍스트 데이터의 벡터화와 토픽 모델링에 대한 학습 내용을 정리한 노트북 모음입니다.     
자연어처리(NLP)에서의 전처리 기법, 텍스트 벡터화 방법, 차원 축소, 그리고 토픽 모델링(LDA/LSA)을 다룹니다.

---

## 학습 노트북 목록

### 1. 단어빈도를 이용한 벡터화
- CountVectorizer, TfidfVectorizer를 활용한 BoW 및 TF-IDF 구현
- DTM(Document-Term Matrix)의 구성 방식 이해
- 단어 빈도 기반의 텍스트 벡터화의 장단점 정리

### 2. 텍스트 분포 기반 비지도 학습 및 토크나이저
- Tokenizer를 사용한 텍스트 시퀀스 처리 및 패딩
- 문장 간 유사도 측정을 위한 코사인 유사도 계산
- 문서 간 유사도 분석

### 3. LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)
- LSA: TruncatedSVD를 통한 차원 축소 후 시각화
- LDA: 문서 내 잠재 토픽 추출 및 시각화
- 두 방식의 개념적 차이점 및 적용 사례 비교

---

## 학습에 대한 최종 정리

아래는 학습한 핵심 내용을 요약한 정리입니다:

- **Bag of Words**: 문서의 단어 분포만을 고려하는 기법으로 문맥 정보 부족의 한계가 있음
- **단어장(Vocabulary)**: 중복 제거된 단어들의 집합. DTM 생성에 사용됨
- **DTM(Document-Term Matrix)**: 문서 내 단어 빈도를 행렬로 표현한 것. 희소 행렬이기 때문에 메모리 비효율적
- **TF-IDF**: 특정 문서에 특화된 단어를 강조하는 가중치 기법. DTM의 단점 보완 가능
- **코사인 유사도**: 문서 벡터 간 방향 유사도를 0~1 범위로 측정
- **LSA vs. LDA**
  - **LSA**: 차원 축소 기법(SVD) 사용, 통계 기반
  - **LDA**: 확률 모델 기반의 주제 추론 기법

(자세한 정리 내용은 `학습에 대한 최종 정리.txt` 파일 참고)

---

## 실행 환경

- Python 3.x
- 주요 라이브러리:
  - pandas
  - sklearn
  - gensim
  - matplotlib, seaborn
  - nltk, konlpy 등 텍스트 전처리용 라이브러리

---

## 참고 사항

- 본 학습은 비지도 학습 기반의 텍스트 분석 기초를 익히기 위한 실습 중심으로 구성되었습니다.
- 예제 텍스트는 주로 한국어 데이터이며, 일부 시각화는 영어 단어 기반 예시로 대체됨.

